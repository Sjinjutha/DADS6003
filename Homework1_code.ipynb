{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8WVwiStfzmJ6jyUPg8g90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sjinjutha/DADS6003/blob/main/Homework1_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ข้อ 1 Normal Equation**"
      ],
      "metadata": {
        "id": "Snhia_4GKy4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "x = np.array([2,5,1,8])\n",
        "y = np.array([12,9,6,7])\n",
        "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "\n",
        "#call inverse function from linear algebra module\n",
        "theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)\n",
        "print(\"theta = \", theta_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn8hZf3fK-Gv",
        "outputId": "84677cb3-4d7a-49af-d23f-50986370c9ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta =  [ 9.16666667 -0.16666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ข้อ 2 Batch GD**"
      ],
      "metadata": {
        "id": "3UadL4eXNbdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pd.DataFrame(\n",
        "    {\n",
        "    'x1' : [2,5,1,8],\n",
        "    'x2' : [3,9,4,5],\n",
        "    'x3' : [6,7,2,3],\n",
        "    }\n",
        ")\n",
        "y = np.array([12,9,6,7])\n",
        "x_b = np.c_[np.ones((4,1)),x]\n",
        "\n",
        "theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)\n",
        "print(\"theta from normal equation = \", theta_best)\n",
        "\n",
        "import random\n",
        "import sklearn\n",
        "from scipy import stats\n",
        "\n",
        "def cost_function(theta, x, y, m):\n",
        "    \n",
        "    y_predict = theta.T.dot(x.T)\n",
        "    #print(\"y_predict \",y_predict)\n",
        "    error = np.sum((y_predict-y)**2)\n",
        "    #print(\"error \", error)\n",
        "    return error\n",
        "\n",
        "def gradient_descent(alpha, x, y, ep=0.0000001, max_iter=3):\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    m = x.shape[0] # number of samples\n",
        "\n",
        "    # initial theta\n",
        "    t = np.ones((x.shape[0],1))\n",
        "    #t = np.ones((x.shape[1],1))\n",
        "    #print(\"t \", t)\n",
        "  \n",
        "    # total error, J(theta)\n",
        "    J = cost_function(t,x,y,m)\n",
        "    #print(\"Iteration 0 --> J=\",J,\" t0=\",t0,\" t1=\",t1)\n",
        "\n",
        "    # Iterate Loop\n",
        "    while not converged:\n",
        "        \n",
        "        #print(\"theta.shape \",t.shape)\n",
        "        #print(\"x.shape \",x.shape)\n",
        "        y_predict = t.T.dot(x.T)\n",
        "        \n",
        "        error = y_predict-y\n",
        "        grad = x.T.dot(error.T)\n",
        "\n",
        "        print(f\"\\nIteration: {iter+1}\")\n",
        "        t = t - alpha * (1/m) * (grad)\n",
        "        print(f\"t0 = {t[0]}, t1 = {t[1]}, t2 = {t[2]}, t3 = {t[3]}\")\n",
        "        \n",
        "        # error\n",
        "        e = cost_function(t,x,y,m)\n",
        "        \n",
        "        if abs(J-e) <= ep:\n",
        "            print(\"Converged, iterations: \", iter, \"/\", max_iter)\n",
        "            converged = True\n",
        "    \n",
        "        J = e   # update error s\n",
        "        iter += 1  # update iter\n",
        "    \n",
        "        if iter == max_iter:\n",
        "            print('\\nMax iterations exceeded!')\n",
        "            converged = True\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    alpha = 0.05 # learning rate\n",
        "    \n",
        "    theta_bgd = gradient_descent(alpha, x_b, y, max_iter=3)\n",
        "    #print (\"\\ntheta \", theta_bgd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0guqisMLR6g",
        "outputId": "864f10e8-b105-485d-88c5-daad7574bf89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta from normal equation =  [ 6.33548387  0.06451613 -0.7483871   1.29677419]\n",
            "\n",
            "Iteration: 1\n",
            "t0 = [0.6875], t1 = [-0.8375], t2 = [-1.1875], t3 = [-0.5625]\n",
            "\n",
            "Iteration: 2\n",
            "t0 = [1.68390625], t1 = [3.57], t2 = [4.50234375], t3 = [4.37984375]\n",
            "\n",
            "Iteration: 3\n",
            "t0 = [-0.85661914], t1 = [-8.66717188], t2 = [-11.04544141], t3 = [-8.37501563]\n",
            "\n",
            "Max iterations exceeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ข้อ 3 Stochastic GD**"
      ],
      "metadata": {
        "id": "a8kORYcGNm6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def st_gradient_descent(alpha, x, y, ep=0.0000001, max_iter=3):\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    m = x.shape[0] # number of samples\n",
        "\n",
        "    # initial theta\n",
        "    t = np.ones((x.shape[0],1))\n",
        "   \n",
        "    # total error, J(theta)\n",
        "    J = cost_function(t,x,y,m)\n",
        "  \n",
        "    i=0\n",
        "    # Iterate Loop\n",
        "    while not converged:\n",
        "        \n",
        "        row = [2,0,3]        \n",
        "        X_i = x[row[i],:]\n",
        "        y_i = y[row[i]].reshape(1,1)\n",
        "\n",
        "        y_predict = t.T.dot(X_i)\n",
        "        error = y_predict-y_i\n",
        "        X_i = X_i.reshape(1,x.shape[1])\n",
        "        grad = X_i.T.dot(error.T)\n",
        "        \n",
        "        #alpha = 0.05 #Learning rate\n",
        "        decay = alpha/max_iter\n",
        "        alpha *= 1/(1+decay*iter)\n",
        "        print(\"\\nlearning schedule = \", alpha)\n",
        "        \n",
        "        print(f\"Iteration: {iter+1}\")\n",
        "        t = t - alpha * (1/1) * (grad)\n",
        "        print(f\"t0 = {t[0]}, t1 = {t[1]}, t2 = {t[2]}, t3 = {t[3]}\")\n",
        "\n",
        "        # error\n",
        "        e = cost_function(t,X_i,y_i,m)\n",
        "        \n",
        "        if abs(J-e) <= ep:\n",
        "            print(\"Converged, iterations: \", iter, \"/\", max_iter)\n",
        "            converged = True\n",
        "    \n",
        "        J = e   # update error s\n",
        "        iter += 1  # update iter\n",
        "        i += 1\n",
        "        \n",
        "        if iter == max_iter:\n",
        "            print('\\nMax iterations exceeded!')\n",
        "            converged = True\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #x, y = make_regression(n_samples=100, n_features=2, n_informative=1, random_state=0, noise=35)\n",
        "    #x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "    \n",
        "    alpha = 0.05 # learning rate\n",
        "\n",
        "    theta_sgd = st_gradient_descent(alpha, x_b, y, max_iter=3)\n",
        "    #print (\"theta \", theta_sgd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I9tAmlhL64y",
        "outputId": "2c26db63-7787-43f8-c99d-2f87ecb12ad5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "learning schedule =  0.05\n",
            "Iteration: 1\n",
            "t0 = [0.9], t1 = [0.9], t2 = [0.6], t3 = [0.8]\n",
            "\n",
            "learning schedule =  0.049180327868852465\n",
            "Iteration: 2\n",
            "t0 = [1.03278689], t1 = [1.16557377], t2 = [0.99836066], t3 = [1.59672131]\n",
            "\n",
            "learning schedule =  0.04761904761904762\n",
            "Iteration: 3\n",
            "t0 = [0.40710383], t1 = [-3.83989071], t2 = [-2.13005464], t3 = [-0.28032787]\n",
            "\n",
            "Max iterations exceeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ข้อ 4 Mini-Batch GD**"
      ],
      "metadata": {
        "id": "2nzc02gHTH00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pd.DataFrame(\n",
        "    {\n",
        "    'x1' : [2,5,1,8],\n",
        "    'x2' : [3,9,4,5],\n",
        "    'x3' : [6,7,2,3],\n",
        "    }\n",
        ")\n",
        "y = np.array([12,9,6,7])\n",
        "x_b = np.c_[np.ones((4,1)),x]\n",
        "\n",
        "def mb_gradient_descent(alpha, x, y, ep=0.00000001, max_iter=3):\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    m = x.shape[0] # number of samples\n",
        "\n",
        "    # initial theta\n",
        "    t = np.ones((x.shape[0],1))\n",
        "   \n",
        "    # total error, J(theta)\n",
        "    J = cost_function(t,x,y,m)\n",
        "    \n",
        "    batch_size = 2\n",
        "  \n",
        "    # Iterate Loop\n",
        "    while not converged:\n",
        "        \n",
        "        #shuffle = decease noise\n",
        "        rand_ind = np.random.permutation(m)\n",
        "        print(f\"\\nrand_ind = {rand_ind}\")\n",
        "        x = x[rand_ind]\n",
        "        y = y[rand_ind]\n",
        "        \n",
        "        for i in range(0,m,batch_size):\n",
        "            print(f\"\\ni = {i}\")\n",
        "            \n",
        "            X_i = x[i:i+batch_size]\n",
        "            print(f\"X_i = {X_i}\")\n",
        "            y_i = y[i:i+batch_size]\n",
        "            print(f\"y_i = {y_i}\")\n",
        "\n",
        "            y_predict = t.T.dot(X_i.T)\n",
        "            error = y_predict-y_i\n",
        "            grad = X_i.T.dot(error.T)\n",
        "\n",
        "            print(f\"Iteration: {iter+1}\")\n",
        "            t = t - alpha * (1/batch_size) * (grad)\n",
        "            print(f\"t0 = {t[0]}, t1 = {t[1]}, t2 = {t[2]}, t3 = {t[3]}\")\n",
        "\n",
        "            # error\n",
        "            e = cost_function(t,X_i,y_i,m)\n",
        "\n",
        "            if abs(J-e) <= ep:\n",
        "                print(\"Converged, iterations: \", iter, \"/\", max_iter)\n",
        "                converged = True\n",
        "\n",
        "            J = e   # update error s\n",
        "            iter += 1  # update iter\n",
        "            \n",
        "            if iter == max_iter:\n",
        "                print('\\nMax iterations exceeded!')\n",
        "                converged = True\n",
        "                break\n",
        "    return t\n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    alpha = 0.05 # learning rate\n",
        "\n",
        "    theta = mb_gradient_descent(alpha, x_b, y, max_iter=3)\n",
        "    #print (\"theta \", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdYB_jJ3OfGZ",
        "outputId": "c16395b0-b0a8-4e2c-e83c-b59e7451758e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "rand_ind = [1 2 3 0]\n",
            "\n",
            "i = 0\n",
            "X_i = [[1. 5. 9. 7.]\n",
            " [1. 1. 4. 2.]]\n",
            "y_i = [9 6]\n",
            "Iteration: 1\n",
            "t0 = [0.625], t1 = [-0.675], t2 = [-2.125], t3 = [-1.375]\n",
            "\n",
            "i = 2\n",
            "X_i = [[1. 8. 5. 3.]\n",
            " [1. 2. 3. 6.]]\n",
            "y_i = [ 7 12]\n",
            "Iteration: 2\n",
            "t0 = [1.971875], t1 = [5.9975], t2 = [3.241875], t3 = [4.716875]\n",
            "\n",
            "rand_ind = [1 3 2 0]\n",
            "\n",
            "i = 0\n",
            "X_i = [[1. 1. 4. 2.]\n",
            " [1. 2. 3. 6.]]\n",
            "y_i = [ 6 12]\n",
            "Iteration: 3\n",
            "t0 = [0.36276562], t1 = [3.38854687], t2 = [-2.19471875], t3 = [-2.50071875]\n",
            "\n",
            "Max iterations exceeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ti62dzoyPBvm"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}